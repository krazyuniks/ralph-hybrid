# Incident Response Skill

> **Use when:** Responding to production incidents, outages, or critical bugs in live systems
> **Generated by:** `ralph-plan` when incident-related patterns detected or explicitly requested

## Purpose

Handle production incidents systematically using an OODA loop pattern (Observe, Orient, Decide, Act). This skill separates speed-critical mitigation from thorough investigation, ensuring incidents are contained quickly while root causes are properly analyzed.

---

## Four-Role Pattern (OODA Loop)

### Role 1: Observer (Situation Analyst)

**Goal:** Rapidly assess the incident scope, impact, and available information.

**Time Budget:** 5-15 minutes MAX

**Tasks:**
1. Identify affected systems and services
2. Determine user/business impact (severity assessment)
3. Gather initial symptoms and error messages
4. Check monitoring dashboards and alerts
5. Note when the incident started
6. Identify any recent changes (deployments, config changes)

**Severity Classification:**
| Severity | Criteria | Response Time |
|----------|----------|---------------|
| **SEV-1** | Complete outage, data loss risk, security breach | Immediate (all hands) |
| **SEV-2** | Major feature broken, significant user impact | < 30 minutes |
| **SEV-3** | Minor feature degraded, workaround available | < 4 hours |
| **SEV-4** | Cosmetic issue, no user impact | Next business day |

**Output Format:**
```markdown
## Incident Observation

### Incident ID: INC-[TIMESTAMP]
**Reported:** [TIME]
**Observer:** [NAME]
**Initial Severity:** SEV-[1-4]

### Affected Systems
| System | Status | Impact |
|--------|--------|--------|
| API Gateway | DEGRADED | 50% requests failing |
| Database | HEALTHY | No issues |
| Auth Service | DOWN | All logins blocked |

### Symptoms
- Error: [exact error message]
- First seen: [timestamp]
- Frequency: [rate]
- Affected users: [count/percentage]

### Recent Changes (Last 24h)
| Time | Change | Author | Likely Related? |
|------|--------|--------|-----------------|
| 14:30 | Deploy v2.3.1 | @dev1 | HIGH |
| 10:00 | Config update | @ops1 | LOW |

### Initial Hypothesis
[Brief theory based on symptoms and recent changes]

### Recommended Next Step
- [ ] ESCALATE to SEV-[X]
- [ ] Proceed to MITIGATE
- [ ] More observation needed: [what]
```

---

### Role 2: Mitigator (Fast Response Agent)

**Goal:** Restore service as quickly as possible, accepting temporary solutions.

**Priority:** SPEED over PERFECTION

**Time Budget:** 15-60 minutes for mitigation (not resolution)

**Mitigation Strategies (in order of preference):**

#### 1. Rollback
```bash
# Deploy previous known-good version
git revert HEAD
kubectl rollout undo deployment/app
docker-compose up -d --no-deps app:v2.3.0

# Feature flag rollback
curl -X POST api/flags/new_feature -d '{"enabled": false}'
```

#### 2. Scale/Restart
```bash
# Horizontal scaling
kubectl scale deployment/app --replicas=5

# Restart pods/containers
kubectl rollout restart deployment/app
docker-compose restart app

# Clear caches
redis-cli FLUSHDB
```

#### 3. Traffic Management
```bash
# Enable maintenance mode
curl -X POST api/maintenance -d '{"enabled": true}'

# Route traffic away from broken component
kubectl patch ingress app --patch '{"spec":{"rules":[...]}}'

# Rate limiting to protect healthy components
nginx -s reload  # with updated rate limits
```

#### 4. Workaround
```bash
# Disable problematic feature
export FEATURE_X_ENABLED=false

# Use fallback service
export PRIMARY_DB_HOST=$FALLBACK_DB_HOST

# Manual data correction (document EVERYTHING)
psql -c "UPDATE users SET status='active' WHERE id=123"
```

**Mitigation Checklist:**
- [ ] Service restored (even partially)
- [ ] Users informed of status
- [ ] Temporary fix documented
- [ ] Monitoring confirms improvement
- [ ] Timeline noted for post-mortem

**Output Format:**
```markdown
## Mitigation Report

### Status: MITIGATED | PARTIALLY_MITIGATED | UNMITIGATED

### Actions Taken
| Time | Action | Result | Reversible? |
|------|--------|--------|-------------|
| 15:01 | Rolled back to v2.3.0 | Service restored | YES |
| 15:05 | Disabled feature flag | Error rate dropped | YES |

### Current Service State
- API: HEALTHY (latency: 50ms, error rate: 0.1%)
- Auth: DEGRADED (manual workaround in place)
- Database: HEALTHY

### Workarounds in Place
1. [Description of workaround]
   - How long can this stay? [time estimate]
   - Risks: [any risks of the workaround]

### Communication Sent
- [ ] Status page updated
- [ ] Customer support notified
- [ ] Stakeholders informed

### Handoff to Investigation
- Symptoms stabilized: YES/NO
- Safe to investigate: YES/NO
- Key areas to investigate: [list]
```

---

### Role 3: Investigator (Root Cause Analyst)

**Goal:** Find the true root cause through thorough, methodical analysis.

**Priority:** THOROUGHNESS over SPEED

**Time Budget:** Hours to days (incident is mitigated, take time to be right)

**Investigation Framework:**

#### Step 1: Timeline Reconstruction
```markdown
## Incident Timeline

| Time | Event | Source | Significance |
|------|-------|--------|--------------|
| 14:00 | Deploy started | CI/CD | Normal |
| 14:05 | Deploy completed | CI/CD | Normal |
| 14:07 | Error rate spike | Datadog | First symptom |
| 14:08 | Alert fired | PagerDuty | Detection |
| 14:10 | On-call paged | PagerDuty | Response start |
```

#### Step 2: Log Analysis
```bash
# Find errors around incident time
grep -A5 -B5 "ERROR\|FATAL\|Exception" app.log | grep "14:0[5-9]"

# Trace specific request that failed
grep "request-id-12345" *.log | sort -t: -k2

# Look for patterns
awk '/ERROR/ {errors[$5]++} END {for (e in errors) print errors[e], e}' app.log | sort -rn | head

# Database slow queries
grep "duration:" postgres.log | awk '$NF > 1000 {print}'
```

#### Step 3: Diff Analysis
```bash
# What changed in the deployment
git diff v2.3.0..v2.3.1

# Config differences
diff -u config.old config.new

# Environment changes
kubectl diff -f deployment.yaml

# Database schema changes
pg_dump --schema-only | diff - schema.old
```

#### Step 4: Hypothesis Testing
```markdown
## Hypothesis Testing

### H1: Database connection exhaustion
**Test:** Check connection pool metrics at incident time
**Evidence:** [data]
**Result:** CONFIRMED | RULED_OUT | PARTIAL

### H2: Memory leak in new code
**Test:** Compare memory usage before/after deploy
**Evidence:** [data]
**Result:** CONFIRMED | RULED_OUT | PARTIAL

### H3: Race condition in auth service
**Test:** Review code changes, check for concurrent access patterns
**Evidence:** [data]
**Result:** CONFIRMED | RULED_OUT | PARTIAL
```

#### Step 5: 5 Whys Analysis
```markdown
## 5 Whys

1. **Why did the service go down?**
   - Answer: API gateway started returning 503s

2. **Why was the API gateway returning 503s?**
   - Answer: Backend pods were crashing

3. **Why were the backend pods crashing?**
   - Answer: Out of memory errors

4. **Why were they running out of memory?**
   - Answer: New feature had a memory leak

5. **Why did the memory leak make it to production?**
   - Answer: Load tests didn't run long enough to detect it

**Root Cause:** Insufficient load test duration for detecting memory leaks
```

**Output Format:**
```markdown
## Investigation Report

### Root Cause
**Summary:** [One sentence description]
**Category:** CODE_BUG | CONFIG_ERROR | INFRASTRUCTURE | EXTERNAL | HUMAN_ERROR
**Confidence:** HIGH | MEDIUM | LOW

### Detailed Analysis
[Multi-paragraph explanation with evidence]

### Timeline
[Detailed timeline with all events]

### Evidence
| Evidence | Source | Significance |
|----------|--------|--------------|
| Memory spike at 14:06 | Grafana | Confirms leak |
| Exception in AuthService | Logs | Shows exact failure |

### Contributing Factors
1. [Factor 1 - and why it mattered]
2. [Factor 2]
3. [Factor 3]

### What Went Well
- Detection time: X minutes (good/bad?)
- Mitigation time: Y minutes
- Communication: [assessment]

### What Went Poorly
- [Area for improvement]
- [Area for improvement]

### Similar Past Incidents
| Incident | Date | Similarity | Learning Applied? |
|----------|------|------------|-------------------|
| INC-123 | 2023-01 | Same service | NO - repeat issue |
```

---

### Role 4: Fixer (Permanent Resolution Agent)

**Goal:** Implement permanent fix and prevent recurrence.

**Priority:** CORRECT and PREVENTIVE

**Fix Categories:**

#### 1. Immediate Code Fix
```markdown
## Code Fix

### Change Description
[What the code change does]

### Files Changed
| File | Change | Risk |
|------|--------|------|
| auth/service.py | Fix memory leak | LOW |
| tests/test_auth.py | Add regression test | NONE |

### Test Plan
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Load test (extended duration) passes
- [ ] Canary deployment successful
```

#### 2. Configuration/Infrastructure Fix
```markdown
## Infrastructure Fix

### Change Description
[What the infrastructure change does]

### Resources Modified
| Resource | Change | Reversible? |
|----------|--------|-------------|
| DB connection pool | Increased from 10 to 50 | YES |
| Pod memory limit | Increased to 2GB | YES |

### Validation
- [ ] Staging environment tested
- [ ] Monitoring updated
- [ ] Runbook updated
```

#### 3. Preventive Measures
```markdown
## Prevention Plan

### Short-term (This Sprint)
- [ ] Add monitoring for [specific metric]
- [ ] Add alert for [condition]
- [ ] Update runbook with mitigation steps

### Medium-term (This Quarter)
- [ ] Implement circuit breaker for [service]
- [ ] Add load test to CI pipeline
- [ ] Review similar code paths

### Long-term (Roadmap)
- [ ] Refactor [component] for reliability
- [ ] Implement [architectural improvement]
- [ ] Training on [topic]
```

**Output Format:**
```markdown
## Fix Report

### Fix Status: DEPLOYED | IN_PROGRESS | PLANNED

### Changes Implemented
| Change | Status | Deployed | Verified |
|--------|--------|----------|----------|
| Memory leak fix | DEPLOYED | 2023-06-15 | YES |
| Extended load tests | MERGED | N/A | YES |
| New alert added | DEPLOYED | 2023-06-15 | YES |

### Verification
- [ ] Fix deployed to production
- [ ] Monitoring confirms improvement
- [ ] No regression detected (24h watch)
- [ ] Post-fix load test passed

### Prevention Measures Added
| Measure | Type | Status |
|---------|------|--------|
| Memory usage alert | MONITORING | ACTIVE |
| 4-hour load test | CI | ACTIVE |
| Runbook update | DOCUMENTATION | COMPLETE |

### Follow-up Items
- [ ] Schedule post-mortem meeting
- [ ] Update incident tracker
- [ ] Share learnings with team
```

---

## Speed vs Thoroughness Matrix

| Role | Speed Priority | Thoroughness Priority | Time Budget |
|------|---------------|----------------------|-------------|
| **Observer** | HIGH | MEDIUM | 5-15 min |
| **Mitigator** | CRITICAL | LOW | 15-60 min |
| **Investigator** | LOW | CRITICAL | Hours-Days |
| **Fixer** | MEDIUM | HIGH | Hours-Days |

**Key Principle:** The Mitigator's job is to STOP THE BLEEDING. Accept duct tape solutions. The Investigator's job is to UNDERSTAND FULLY. Take all the time needed. Don't mix these concerns!

---

## Incident Response Output Format

Generate `INCIDENT-RESPONSE.md`:

```markdown
# Incident Response Report

**Incident ID:** INC-[TIMESTAMP]
**Date:** [DATE]
**Duration:** [START] to [END] ([TOTAL TIME])
**Severity:** SEV-[1-4]
**Status:** RESOLVED | MONITORING | ONGOING

## Executive Summary

**Impact:** [Brief description of user/business impact]
**Root Cause:** [One sentence]
**Resolution:** [One sentence]
**Time to Detect:** [X minutes]
**Time to Mitigate:** [Y minutes]
**Time to Resolve:** [Z hours/days]

## Roles Completed

| Role | Assignee | Status | Duration |
|------|----------|--------|----------|
| Observer | [NAME] | COMPLETE | 10 min |
| Mitigator | [NAME] | COMPLETE | 25 min |
| Investigator | [NAME] | COMPLETE | 4 hours |
| Fixer | [NAME] | COMPLETE | 2 days |

## Observation Summary
[Key points from Role 1]

## Mitigation Summary
[Key points from Role 2]

## Investigation Summary
[Key points from Role 3]

## Fix Summary
[Key points from Role 4]

## Action Items

### Immediate (Completed)
- [x] Service restored
- [x] Root cause identified
- [x] Fix deployed

### Short-term (This Week)
- [ ] Post-mortem scheduled
- [ ] Runbook updated
- [ ] Monitoring improved

### Long-term (This Quarter)
- [ ] Architectural improvement
- [ ] Process improvement

## Lessons Learned

### What Worked Well
1. [Item]
2. [Item]

### What Could Be Improved
1. [Item]
2. [Item]

### Recommendations
1. [Recommendation]
2. [Recommendation]

## Sign-Off

| Role | Status | Notes |
|------|--------|-------|
| Observer | COMPLETE | [summary] |
| Mitigator | COMPLETE | [summary] |
| Investigator | COMPLETE | [summary] |
| Fixer | COMPLETE | [summary] |
```

---

## Quick Incident Commands

```bash
# Check service health
curl -s http://service/health | jq .

# Get recent errors from logs
journalctl -u app --since "1 hour ago" | grep -i error | tail -50

# Check resource usage
kubectl top pods -n production
docker stats --no-stream

# Recent deployments
kubectl rollout history deployment/app
git log --oneline --since="1 day ago"

# Database connections
psql -c "SELECT count(*) FROM pg_stat_activity;"

# Check for recent config changes
git log --oneline -10 -- config/
kubectl get configmap app-config -o yaml

# Network connectivity
curl -w "@curl-format.txt" -s -o /dev/null http://dependency-service/health

# Memory/CPU pressure
free -h && uptime
kubectl describe node | grep -A5 "Allocated resources"
```

---

## Communication Templates

### Initial Alert (Observer → Team)
```
INCIDENT: [Brief description]
SEVERITY: SEV-[X]
IMPACT: [Who/what is affected]
STATUS: Investigating
LEAD: [Name]
CHANNEL: #incident-[id]
```

### Status Update (Mitigator → Stakeholders)
```
UPDATE: [Incident name]
STATUS: Mitigation in progress
CURRENT STATE: [Service status]
ETA: [If known]
WORKAROUND: [If any]
NEXT UPDATE: [Time]
```

### Resolution Notice (Fixer → All)
```
RESOLVED: [Incident name]
DURATION: [Total time]
ROOT CAUSE: [One sentence]
FIX: [What was done]
POST-MORTEM: [Scheduled date/time]
```

---

## When to Trigger This Skill

Automatically activate for:
- Production alerts firing
- User-reported outages
- Error rate spikes
- Performance degradation
- Security incidents
- Data integrity issues
- Dependency failures

Keywords that trigger: `incident`, `outage`, `down`, `broken`, `production`, `alert`, `emergency`, `urgent`, `critical`, `SEV-1`, `SEV-2`, `on-call`, `page`, `escalate`

---

## Integration with Other Skills

### With Code Archaeology
For incidents in legacy systems:
1. Mitigator: Restore service first
2. Archaeologist: Understand the legacy code
3. Investigator: Find root cause with archaeology context
4. Fixer: Make safe changes following archaeology guidance

### With Adversarial Review
For security incidents:
1. Observer: Assess security impact
2. Mitigator: Contain the threat
3. Adversarial Review: Security analysis
4. Fixer: Security hardening

### With Debug Agent
For complex root cause analysis:
1. Investigator identifies candidate causes
2. Debug Agent systematically tests hypotheses
3. Evidence from debugging informs fix

---

## Post-Incident Checklist

- [ ] Incident documented in tracker
- [ ] Timeline recorded accurately
- [ ] Root cause confirmed (not guessed)
- [ ] Fix deployed and verified
- [ ] Monitoring/alerting improved
- [ ] Runbook updated
- [ ] Post-mortem scheduled
- [ ] Action items assigned
- [ ] Stakeholders notified of resolution
- [ ] Similar incidents reviewed for patterns

---

## References

- OODA Loop: https://en.wikipedia.org/wiki/OODA_loop
- Google SRE Book - Incident Management: https://sre.google/sre-book/managing-incidents/
- PagerDuty Incident Response: https://response.pagerduty.com/
- Blameless Post-Mortems: https://www.atlassian.com/incident-management/postmortem/blameless
